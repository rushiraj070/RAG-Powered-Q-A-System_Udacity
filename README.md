ğŸš€ Contextual AI Q&A System
An intelligent Question Answering system leveraging OpenAI embeddings and RAG for precise answers from custom knowledge bases.

âœ¨ Demo
(Replace this section with a GIF or screenshot showing your system in action!)

Here's a quick look at how the system processes a question and retrieves relevant answers:

ğŸ’¡ Problem Statement
Navigating through large volumes of unstructured text to find specific answers can be time-consuming and inefficient. Traditional keyword search often falls short, missing contextually relevant information. This project addresses this challenge by enabling users to ask natural language questions and receive accurate, targeted answers directly from a provided knowledge base.

ğŸ§  Solution Overview: Retrieval-Augmented Generation (RAG)
This system is built upon a Retrieval-Augmented Generation (RAG) architecture. Here's how it works:

Embedding Generation: Each document (or chunk of text) in the knowledge base is converted into a high-dimensional numerical representation called a vector embedding using OpenAI's text-embedding-3-small model. These embeddings capture the semantic meaning of the text.

Semantic Search: When a user asks a question, its embedding is generated. This question embedding is then compared against all document embeddings using cosine similarity to find the most semantically relevant pieces of information from the knowledge base.

Contextual Prompting: The top-ranked, most relevant document snippets are then combined with the user's original question to form a comprehensive prompt.

LLM Inference: This context-rich prompt is fed to a powerful Large Language Model (LLM) like gpt-3.5-turbo. The LLM then generates a concise and accurate answer based only on the provided context, minimizing hallucinations and ensuring factual grounding.

ğŸŒŸ Key Features
Contextual Question Answering: Provides precise and accurate answers by retrieving information from a custom knowledge base.

Advanced Semantic Search: Utilizes state-of-the-art OpenAI embedding models for highly relevant context retrieval.

Dynamic Contextualization: Automatically identifies, ranks, and assembles the most pertinent document snippets to inform the LLM.

Efficient Token Management: Integrates tiktoken to intelligently handle and respect token limits for both embedding and LLM calls, optimizing API usage.

Modular Python Implementation: Designed with clean, maintainable, and reusable Python code.

Scalable Architecture: Easily adaptable to larger datasets and different OpenAI models.

ğŸ› ï¸ Technologies Used
Category

Technology

Badge

Language

Python

AI/ML

OpenAI API

LLM

gpt-3.5-turbo

Embeddings

text-embedding-3-small

Data Mgmt.

Pandas

Numerical

NumPy

Similarity

SciPy

Tokenization

tiktoken

Environment

Jupyter Notebook

ğŸš€ Quickstart
Follow these steps to set up and run the project locally.

Prerequisites
Python 3.8+

An OpenAI API Key.

1. Clone the Repository
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

(Remember to replace your-username/your-repo-name.git and your-repo-name with your actual GitHub repository details.)

2. Set Up Virtual Environment
It's highly recommended to use a virtual environment to manage dependencies:

python -m venv venv
# On Windows
.\venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate

3. Install Dependencies
pip install -r requirements.txt

(Ensure you have a requirements.txt file generated from your environment, e.g., using pip freeze > requirements.txt)

4. Set Your OpenAI API Key
Set your OpenAI API key as an environment variable. This is the most secure way to handle your API key.

On macOS/Linux:

export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

On Windows (Command Prompt):

set OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

On Windows (PowerShell):

$env:OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

(Important: Replace "YOUR_OPENAI_API_KEY" with your actual OpenAI API Key. Do not commit your API key directly to your code or GitHub! If you use a custom base URL like https://openai.vocareum.com/v1, you might hardcode it in the client initialization or add it as another environment variable.)

5. Prepare Your Data
A sample dataset (india_2024_events_sample.csv) should be available in the data/ directory. This is a small subset of the data used for the demonstration.

6. Run the Demo
You can run the interactive demo via a Jupyter Notebook or a Python script:

Option A: Jupyter Notebook (for interactive exploration)
jupyter notebook notebooks/project_exploration.ipynb

(Open the project_exploration.ipynb notebook and run the cells sequentially.)

Option B: Python Script (for quick execution)
python src/main.py

(This assumes you have a main.py script that orchestrates the Q&A process. If you don't have one, create it by putting the client initialization, create_prompt, answer_question functions, and example usage there.)

ğŸ“ Project Structure
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ india_2024_events_sample.csv  # Sample dataset for demonstration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedding_utils.py # Functions for embedding generation and similarity calculations
â”‚   â”œâ”€â”€ rag_pipeline.py    # Core RAG logic: prompt creation, answer generation
â”‚   â””â”€â”€ main.py            # Main script to run the Q&A demo
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ project_exploration.ipynb # Jupyter Notebook with detailed code exploration
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ README.md                # This file

ğŸ“ˆ Future Enhancements
User Interface: Develop a more interactive web UI using frameworks like Streamlit or Gradio for a better user experience.

Scalability: Integrate with vector databases (e.g., Pinecone, Weaviate, ChromaDB) for efficient storage and retrieval of millions of embeddings.

Error Handling: Implement more robust error handling and logging for production readiness.

Evaluation Metrics: Add quantitative evaluation metrics to assess the accuracy and relevance of generated answers.

More Models: Experiment with different embedding and LLM models (e.g., open-source models) to compare performance and cost.
